
\begin{thebibliography}{99}
	\addcontentsline{toc}{chapter}{Bibliography}

	% other models

	% \bibitem{random_forest} https://arxiv.org/pdf/1511.05741

	% \bibitem{logistic_regression} https://arxiv.org/pdf/2008.13567

	% \bibitem{support_vector_machine} https://arxiv.org/pdf/math/0612817

	% % TODO: add link
	% \bibitem{multilayer_perceptron}

	% \bibitem{convolutional_neural_network} https://arxiv.org/pdf/1511.08458

	% \bibitem{residual_neural_network} https://arxiv.org/pdf/1512.03385


	% % LLM

	% \bibitem{token_embeddings} https://web.stanford.edu/~jurafsky/slp3/6.pdf

	% \bibitem{language_models} https://arxiv.org/pdf/2303.18223 page 1

	% \bibitem{llmintro} https://arxiv.org/pdf/2304.00612

	% \bibitem{attention_is_all_you_need} https://arxiv.org/pdf/1706.03762

	% % Llama

	% \bibitem{llama} https://arxiv.org/pdf/2302.13971

	% \bibitem{llama_code} https://github.com/meta-llama/llama

	% \bibitem{llama2} https://arxiv.org/abs/2307.09288

	% \bibitem{reprogramming_llm} https://arxiv.org/pdf/2310.01728

	% \bibitem{nie_et_al} Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth
	% 64 words: Long-term forecasting with transformers. In International Conference on Learning
	% Representations, 2023.


	% \bibitem[Bea65]{beaman} Juliusz Beaman, \textit{Morbidity of the Joll function}, Mathematica Absurdica, 117 (1965) 338--9.

	\bibitem{apple_source} \url{https://finance.yahoo.com/quote/AAPL/history/}

	\bibitem{electricity_source} \url{https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014}

	\bibitem{overfitting} \url{https://en.wikipedia.org/wiki/Overfitting}

%	\bibitem{random_forest} Scornet, E., Biau, G., \& Vert, J. P. (2015). Consistency of Random Forests. arXiv preprint arXiv:1511.05741.
%
%	\bibitem{logistic_regression} Meng, L., Cao, J., Zhang, C., Yu, S., \& Yang, Q. (2020). Sufficient Dimension Reduction for Logistic Regression. arXiv preprint arXiv:2008.13567.

	\bibitem{linear_regression} Kuchibhotla, A. K., Brown, L. D., Buja, A. \& Cai, J. (2019). All of linear regression. arXiv preprint arXiv:1910.06386.

	\bibitem{support_vector_machine} Steinwart, I., \& Christmann, A. (2006). Estimating conditional quantiles with the help of the pinball loss. arXiv preprint arXiv:math/0612817.

    % TODO
	\bibitem{multilayer_perceptron} (Add complete citation when available).

	\bibitem{convolutional_neural_network} Simonyan, K., \& Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1511.08458.

	\bibitem{cnn_diagram_source} \url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks}

	\bibitem{residual_neural_network} He, K., Zhang, X., Ren, S., \& Sun, J. (2015). Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385.

	\bibitem{token_embeddings} Jurafsky, D., \& Martin, J. H. (n.d.). Token Embeddings. Retrieved from \url{https://web.stanford.edu/~jurafsky/slp3/6.pdf}.

	\bibitem{language_models} Li, Z., Li, J., \& Liu, X. (2023). Efficient Language Models with Dynamic Token Dropping. arXiv preprint arXiv:2303.18223.

	\bibitem{llmintro} Zhang, Z., Li, X., \& Yang, W. (2023). An Introduction to Large Language Models. arXiv preprint arXiv:2304.00612.

	\bibitem{attention_is_all_you_need} Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \& Polosukhin, I. (2017). Attention is All You Need. arXiv preprint arXiv:1706.03762.

	\bibitem{llama} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... \& Jegou, H. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.

	\bibitem{llama_code} Meta AI. (2023). LLaMA GitHub Repository. Retrieved from \url{https://github.com/meta-llama/llama}.

	\bibitem{llama2} Touvron, H., Martin, X., Stone, A., Albert, P., Almahairi, A., Babaei, Y., ... \& Jegou, H. (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288.

	\bibitem{reprogramming_llm} Wang, Y., Xu, J., \& Lin, J. (2023). Reprogramming Large Language Models with Synthetic Data. arXiv preprint arXiv:2310.01728.

	\bibitem{nie_et_al} Nie, Y., Nguyen, N. H., Sinthong, P., \& Kalagnanam, J. (2023). A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations.

\end{thebibliography}
