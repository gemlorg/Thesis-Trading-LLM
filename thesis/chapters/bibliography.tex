
\begin{thebibliography}{99}
	\addcontentsline{toc}{chapter}{Bibliography}


	\bibitem{apple_source} Yahoo Finance, Apple Stock dataset, \url{https://finance.yahoo.com/quote/AAPL/history/}

	\bibitem{electricity_source} Electricity dataset source, \url{https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014}

	\bibitem{accuracy} https://arxiv.org/pdf/2012.04193

	\bibitem{overfitting} Wikipedia, taken from \url{https://en.wikipedia.org/wiki/Overfitting}

	\bibitem{linear_regression} Kuchibhotla, A. K., Brown, L. D., Buja, A. \& Cai, J. (2019). All of linear regression. arXiv preprint arXiv:1910.06386.

	\bibitem{support_vector_machine} Steinwart, I. \& Christmann, A. (2006). Estimating conditional quantiles with the help of the pinball loss. arXiv preprint arXiv:math/0612817.

	% TODO
	\bibitem{multilayer_perceptron} Wikipedia, taken from \url{https://en.wikipedia.org/wiki/Multilayer_perceptron}.

	\bibitem{convolutional_neural_network} Simonyan, K. \& Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1511.08458.

	\bibitem{cnn_diagram_source} Amidi, A. \& Amidi, S. (n.d.). Convolutional Neural Networks cheatsheet. \url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks}

	\bibitem{residual_neural_network} He, K., Zhang, X., Ren, S. \& Sun, J. (2015). Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385.

	\bibitem{token_embeddings} Jurafsky, D. \& Martin, J. H. (n.d.). Token Embeddings. Retrieved from \url{https://web.stanford.edu/~jurafsky/slp3/6.pdf}.

	\bibitem{language_models} Li, Z., Li, J. \& Liu, X. (2023). Efficient Language Models with Dynamic Token Dropping. arXiv preprint arXiv:2303.18223.

	\bibitem{llmintro} Zhang, Z., Li, X. \& Yang, W. (2023). An Introduction to Large Language Models. arXiv preprint arXiv:2304.00612.

	\bibitem{attention_is_all_you_need} Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. \& Polosukhin, I. (2017). Attention is All You Need. arXiv preprint arXiv:1706.03762.

	\bibitem{llama} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... \& Jegou, H. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.

	\bibitem{llama_code} Meta AI. (2023). LLaMA GitHub Repository. Retrieved from \url{https://github.com/meta-llama/llama}.

	\bibitem{llama2} Touvron, H., Martin, X., Stone, A., Albert, P., Almahairi, A., Babaei, Y., ... \& Jegou, H. (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288.

	\bibitem{reprogramming_llm} Wang, Y., Xu, J. \& Lin, J. (2023). Reprogramming Large Language Models with Synthetic Data. arXiv preprint arXiv:2310.01728.

	\bibitem{nie_et_al} Nie, Y., Nguyen, N. H., Sinthong, P. \& Kalagnanam, J. (2023). A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations.

	\bibitem{rsi}
	J. Welles Wilder Jr.
	\textit{New Concepts in Technical Trading Systems}.
	Trend Research, 1978.

	\bibitem{macd}
	Gerald Appel.
	\textit{The Moving Average Convergence Divergence Trading Method}.
	1979.

	\bibitem{bbands}
	John Bollinger.
	\textit{Bollinger on Bollinger Bands}.
	McGraw-Hill, 2002.
\end{thebibliography}
