In this chapter we present the basic theory behind Large Language Models. We then introduce the LLaMA family of models, which we've been using. Subsequently, we describe the embedding technique we've used and we present our results.

\section{Vocabulary}

The following vocabulary is used:
\begin{itemize}
	\item \textbf{Token} is a basic unit of text data a language model processes - usually words, subwords, punctuation marks etc.

	\item \textbf{Tokenization} is a process of breaking down input data into tokens.

	\item \textbf{Language model} is a \textit{probabilistic model} of a natural language. Probabilistic - meaning that given some input text data, it's job is to predict the future token. \cite{language_models}

	\item \textbf{Supervised learning} is a type of a method of training machine learning models where every input is supplied with the output the model is expected to produce. The model then matches its own output with the expected output to correct its own behaviour.

	\item \textbf{Pretraining} is a process of training the language model on a corpus of data

	\item \textbf{Fine-tuning} is a process of adapting a pretrained language model to a specific task (e.g. mathematics, poetry) by training it on a smaller, task-specific dataset.

	\item \textbf{Token embedding} is a mapping of tokens to high-dimensional vectors of real numbers. This mapping is expected to have the property that tokens similar in meaning are close in the output space. See \cite{token_embeddings}.

	\item \textbf{Context length} is the maximal size of input tokens a large language model can process at any one time.

	\item \textbf{Cross-modality data} is data that combines multiple modalities, i.e. text, image, audio, video, etc. In particuliar, combination of text description and time series is cross-modality data.

\end{itemize}
% CLEAN

\section{Overview}

A Large Language Model (LLM) \cite{llmintro} is a language model that is pretrained on a large collection of data (usually millions of tokens).
%an advanced artificial intelligence system designed to understand and generate human-like text based on patterns it learns from enormous amounts of textual data.
These models utilize deep learning techniques, particularly neural networks, which consist of interconnected neuron layers that process information sequentially, one by one.
The predominant architecture underpinning most contemporary LLMs is the Transformer (see below), notable for its self-attention mechanism that enables the model to assess the importance of different tokens in a sentence irrespective of the order the tokens are in.

The training of an LLM involves pretraining it with a large, diverse corpus of text, during which it adjusts its internal parameters to minimize the difference between its predictions and the actual data. This process of supervised learning equips the model with a probabilistic understanding of the language (its patterns, semantics, syntax, knowledge of the world inherent in a language), enabling it to predict a continuation of a given piece of input text.

Once trained, LLMs can perform a variety of language-based tasks such as translation, summarization, question answering, and text generation.
It can then be fine-tuned to perform better on a specific task, e.g. write poetry, give cooking advice, write programming code, etc.
%Their capability to generate coherent and contextually relevant text makes them invaluable for applications in natural language processing (NLP), including conversational agents, content creation tools, and sophisticated text analysis. 
We will see how such a model deals with analysing and predicting time series data.


\section{The Transformer}

The Transformer is a type of neural network architecture introduced in the seminal 2017 paper \textit{"Attention is All You Need"} by Vaswani et al. \cite{attention_is_all_you_need}. It has since become foundational for many natural language processing (NLP) models due to its efficiency and effectiveness in handling data sequences, such as text.

\subsection{Components}

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{"pictures/the-transformer-model-architecture.png"} % TODO: better diagram
	\caption{The Transformer model architecture} % TODO: better description - every part of the diagram has to be explained
	\label{fig:transformer_architecture_fig}
\end{figure}

The Transformer (fig \autoref{fig:transformer_architecture_fig}) consists of the following components:

\begin{enumerate}
	\item \textbf{Input Embedding}: Converts input tokens into vectors
	\item \textbf{Positional Encoding}: Adds positional information to the embeddings to retain the order of the sequence. This encoding is another high-dimensional vector.
	\item \textbf{Encoder} (\autoref{fig:transformer_architecture_fig} the left):
	      \begin{itemize}
		      \item Consists of \(N = 6\)\footnote{\cite{attention_is_all_you_need}, section 3.1)} layers.
		      \item Each layer has two sub-layers:
		            \begin{itemize}
			            \item \textbf{Multi-Head Attention}: Applies attention mechanism over the input. For each token calculates the weight or importance of over tokens in the surrounding context (\textit{Attention}). Does so independently with \(h\)\footnote{In \cite{attention_is_all_you_need}, \(h = 6\)} 'heads', each calculating different semantic relantionships (\textit{Multi-Head}). The results are concatenated and linearly transformed into size of one output (as if from one head).
			            \item \textbf{Feed Forward}: Applies a fully connected feed-forward neural network.
		            \end{itemize}
		      \item \textbf{Add \& Norm}: Residual connections followed by layer normalization after each sub-layer.
	      \end{itemize}
	\item \textbf{Decoder} (\autoref{fig:transformer_architecture_fig}, on the right):
	      \begin{itemize}
		      \item Also consists of \(N = 6\)\footnote{\cite{attention_is_all_you_need}, section 3.1)} layers.
		      \item Each layer has three sub-layers:
		            \begin{itemize}
			            \item \textbf{Masked Multi-Head Attention}: As in Encoder, but masked, i.e. attention results for future tokens are disarded - .
			            \item \textbf{Multi-Head Attention}: Applies the multi-head attention mechanism to encoder output.
			            \item \textbf{Feed Forward}: Applies a fully connected feed-forward neural network.
		            \end{itemize}
		      \item \textbf{Add \& Norm}: Residual connections followed by layer normalization after each sub-layer.
	      \end{itemize}
	\item \textbf{Output Embedding}: Converts decoder output tokens into semantic vector space.
	\item \textbf{Linear \& Softmax Layer}: Maps the decoder’s output to the probability distribution over the target vocabulary. The most probable token is the output. \footnote{This may seem contrary to popular experience using e.g. ChatGPT, where given the same input, it may not necessarily output the same result. However, such tools may have random seeds for each interaction session and also may take into account the context of the conversation. }
\end{enumerate}




\section{LLaMA model}

\subsection{Introduction}
\textbf{LLaMA} or \textit{Large Language Model Meta AI} \cite{llama} is a collection of large language models developed by Meta AI.

\subsection{Features}

\begin{itemize}
	\item \textbf{Model Variants:} LLaMA is available in various sizes, offering flexibility for deployment in different environments. These variants range from models of 7 billion parameters in size up to models of 65 billion parameters in size. \footnote{LLaMa-3, which came out in April 2024, has size possibilities of 8 billion and 70 billion parameters}

	\item \textbf{Training Data:} The model has been trained on 1.4 trillion tokens of data from several sources, including CommonCrawl and Github, Wikipedia (Table 1. in \cite{llama}). It therefore has an enormous and domain diverse range of input data.

	      % \item \textbf{Application Scope:} Due to the possibility of using different sizes, LLaMA is suited for a variety of applications, such as conversational agents, content generation, summarization, and more advanced tasks like sentiment analysis and machine translation.

	\item \textbf{Accessibility:} The code that can be used to run the model has been publicly released under the open-source GPLv3 license \cite{llama_code}.
\end{itemize}

\subsection{LLaMA-2}
\textbf{LLaMA-2} \cite{llama2} is an improved version of LLaMA, with similar model sizes. It has the same architecture as LLaMA-1, but was trained on a much larger set of data (~2 trillion tokens). It also has doubled context length of 4096 tokens.

\section{Time-series Embedding}
We now present the technique we've used for using a vanilla (not fine-tuned) LLaMA-2 model to predict time series data.
The main idea involves using a framework around a frozen LLM (i.e. one that is not changed during the training process) that transforms input time series data into a text representation the LLM can then work on. Its output is then converted into a prediction. The idea is due to an article by Jin et al. (2024) \cite{reprogramming_llm}.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{"pictures/prompt-embedding.png"} % TODO: poor quality of picture - should be better explained
	\caption{The model framework of TIME-LLM. Given an input time series, we first tokenize and
		embed it via (1) patching along with a (2) customized embedding layer. (3) These patch embeddings
		are then reprogrammed with condensed text prototypes to align two modalities. To augment the
		LLM’s reasoning ability, (4) additional prompt prefixes are added to the input to direct the transformation
		of input patches. (5) The output patches from the LLM are projected to generate the forecasts.}
	\label{fig:prompt_embedding_fig}
\end{figure}



The following three paragraphs come from the article.
% TODO: This paragraph/problem may also be described earlier, e.g. in introduction.
We consider the following problem: given a sequence of historical observations \(X \in \R^{N\times T}\)
consisting of \(N\) different 1-dimensional variables across \(T\) time steps, we aim to reprogram a large
language model \(f(\cdot)\) to understand the input time series and accurately forecast the readings at \(H\) future time steps, denoted by \(\hat{Y} \in \R^{N\times H}\) , with the overall objective to minimize the mean square errors between the expected outputs \(Y\) and predictions, i.e., \(\frac1H \sum_{h=1}^H \| \hat{Y}_h - Y_h \|_F^2 \).

% TODO: normalization, patching, embedding - explain
The method encompasses three main components: (1) input transformation, (2) a pre-trained and frozen LLM, and (3) output projection. Initially, a multifeature time series is partitioned into \(N\) unifeature time series, which are subsequently processed independently (Nie et al., 2023) \cite{nie_et_al}.
The i-th series is denoted as \(X(i) \in \R^{1\times T}\) , which undergoes normalization, patching, and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities.
Then, we augment the LLM’s time series reasoning ability by prompting it together with the transformed series to generate output representations, which are projected to the final forecasts \(\hat{Y}^{(i)} \in \R^{1\times H}\) .

We note that only the parameters of the lightweight input transformation and output projection are updated, while the backbone language model is frozen.
In contrast to vision-language and other multimodal language models, which usually fine-tune with paired cross-modality data, this use of model is directly optimized and becomes readily available with only a small set of time series and a few  training epochs, maintaining high efficiency and imposing fewer resource constraints compared to building large domain-specific models from scratch or fine-tuning them.

\section{Our methodology}
\section{Model Parameters}

In this study, we employed a set of distinctive parameters for our model training:

\begin{itemize}
    \item \textbf{seq\_len} – This parameter defines the number of records in one prompt to the model.
    \item \textbf{pred\_len} – This parameter specifies the number of records to predict.
    \item \textbf{seq\_step} – This parameter determines the step size in records to move the prompt. For instance, if \textit{seq\_step} is set to 4, the records would be indexed as 0, 4, 8, and so on. This approach was essential in filtering out noise caused by hourly fluctuations, significantly enhancing the results.
\end{itemize}

\subsection{Impact on Results}
The effectiveness of these parameters is highly contingent upon the specific dataset in use. For datasets characterized by high volatility and a high frequency of records, a smaller \textit{seq\_step} is preferable. Conversely, for data that remains relatively stable over time, a larger \textit{seq\_step} is necessary to prevent the model from merely replicating the last observed value.

Our experimentation with various proportions between \textit{seq\_len} and \textit{pred\_len} revealed that optimal results were achieved when \textit{pred\_len} was approximately one-fourth of \textit{seq\_len}. This finding is intuitive, as it ensures the indicators retain their significance. A more detailed discussion on this can be found in the Prompt Engineering section.

Due to constraints in time and resources, we were unable to identify a universally optimal ratio for all datasets. Nevertheless, we believe that such a golden ratio exists and can be discovered with further research. More detailed information on this can be found in the Results and Conclusion section.

\subsection{Overfitting Concerns}
To avoid reducing the number of records available for training, we leveraged the \textit{seq\_step} parameter in our data loaders. Rather than using every \textit{seq\_step} value, we trained the model with sequences such as 0, 4, 8, 12, etc., followed by 1, 5, 9, 13, and so on (if \textit{seq\_step} was set to 4).

This approach, however, led to overfitting, particularly with a high \textit{seq\_len} of 200, a \textit{seq\_step} of 12, and a \textit{pred\_len} of 40. For illustration, consider feeding the model data from the past 20 weeks and predicting the next 4 weeks (one month). Our currency datasets contain 24 records per day over 5 days a week.

In the initial iterations, our model achieved an accuracy of 89% on the training data in the first epoch. This high accuracy was due to the similarity of the training data sequences. Essentially, we presented price data for the last 5 months divided into 200 records and then predicted the price for the next month. We then slightly shifted the data forward by 1 hour, resulting in nearly identical sequences being fed to the model. Consequently, the model could easily predict the next month +1 hour, given its similarity to the previous predictions.

However, this approach failed during validation, as the model's accuracy drastically dropped when applied to completely unseen test data. This highlights the importance of diversifying training sequences to avoid overfitting and improve the model's generalization capabilities. Further exploration and refinement of these parameters are necessary to develop a robust predictive model.

\section{Prompt Engineering}

The foundational concept behind leveraging a Large Language Model (LLM) lies in its extensive knowledge about the world. Our objective was to determine optimal strategies to harness this knowledge, thereby enhancing our model's forecasting accuracy. Essentially, we aimed to bypass fine-tuning and instead focus on crafting the most effective task descriptions to elicit accurate predictions from the outset.

Initially, we experimented with simply providing sequences of numbers, similar to our approach in other models. However, this method proved inadequate, as the responses were often off-target and lacked focus. Recognizing the need for a more sophisticated approach, we turned to autocorrelation analysis to identify recurring patterns within the data. By applying Fourier transformation, we identified the most significant patterns and selected the steps with the highest autocorrelation as our top_k features.

This method notably improved our accuracy, especially for datasets with clear seasonal patterns, such as electricity consumption over the year. For instance, electricity usage typically increases during the winter months and decreases during the summer, with similar trends repeating annually. However, our primary goal was to predict market movements, particularly in the forex market, which lacks such seasonal patterns. Stock prices, for example, tend to grow over the years rather than oscillate within a fixed range.

To address this challenge, we explored various analytical tools commonly used in trading. Numerous indicators are designed to forecast future prices, based on factors such as moving averages, trading volume, and price trends. We decided to incorporate three widely-used indicators: Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), and Bollinger Bands (BBANDS). These indicators, already familiar to the model due to its pre-existing knowledge, significantly enhanced its predictive capabilities.

\subsection{Indicators Used}

\begin{itemize}
    \item \textbf{Relative Strength Index (RSI)}: This momentum oscillator measures the speed and change of price movements. It oscillates between 0 and 100 and is typically used to identify overbought or oversold conditions in a market.
    \item \textbf{Moving Average Convergence Divergence (MACD)}: This trend-following indicator shows the relationship between two moving averages of a security's price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.
    \item \textbf{Bollinger Bands (BBANDS)}: These volatility bands are placed above and below a moving average. Volatility is based on the standard deviation, which changes as volatility increases and decreases.
\end{itemize}

Incorporating these indicators yielded a substantial improvement in our model's performance. The accuracy of our predictions increased by over 2% on certain datasets, which is a significant enhancement in our context. This improvement underscores the value of integrating domain-specific indicators and leveraging the LLM's pre-existing knowledge to refine our forecasting model. Further detailed results will be discussed in the subsequent sections.

\section{Results}
