In this chapter we present the basic theory behind Large Language Models. We then introduce the LLaMA family of models, which we've been using. Subsequently, we describe the embedding technique we've used and we present our results.

\section{Vocabulary}

The following vocabulary is used:
\begin{itemize}
	\item \textbf{Token} is a basic unit of text data a language model processes - usually words, subwords, punctuation marks etc.

	\item \textbf{Tokenization} is a process of breaking down input data into tokens.

	\item \textbf{Language model} is a \textit{probabilistic model} of a natural language. Probabilistic - meaning that given some input text data, it's job is to predict the future token. \cite{language_models}

	\item \textbf{Supervised learning} is a type of a method of training machine learning models where every input is supplied with the output the model is expected to produce. The model then matches its own output with the expected output to correct its own behaviour.

	\item \textbf{Pretraining} is a process of training the language model on a corpus of data

	\item \textbf{Fine-tuning} is a process of adapting a pretrained language model to a specific task (e.g. mathematics, poetry) by training it on a smaller, task-specific dataset.

	\item \textbf{Token embedding} is a mapping of tokens to high-dimensional vectors of real numbers. This mapping is expected to have the property that tokens similar in meaning are close in the output space. See \cite{token_embeddings}.

	\item \textbf{Context length} is the maximal size of input tokens a large language model can process at any one time.

\end{itemize}
% CLEAN

\section{Overview}

A Large Language Model (LLM) \cite{llmintro} is a language model that is pretrained on a large collection of data (usually millions of tokens).
%an advanced artificial intelligence system designed to understand and generate human-like text based on patterns it learns from enormous amounts of textual data.
These models utilize deep learning techniques, particularly neural networks, which consist of interconnected neuron layers that process information sequentially, one by one.
The predominant architecture underpinning most contemporary LLMs is the Transformer (see below), notable for its self-attention mechanism that enables the model to assess the importance of different tokens in a sentence irrespective of the order the tokens are in.

The training of an LLM involves pretraining it with a large, diverse corpus of text, during which it adjusts its internal parameters to minimize the difference between its predictions and the actual data. This process of supervised learning equips the model with a probabilistic understanding of the language (its patterns, semantics, syntax, knowledge of the world inherent in a language), enabling it to predict a continuation of a given piece of input text.

Once trained, LLMs can perform a variety of language-based tasks such as translation, summarization, question answering, and text generation.
It can then be fine-tuned to perform better on a specific task, e.g. write poetry, give cooking advice, write programming code, etc.
%Their capability to generate coherent and contextually relevant text makes them invaluable for applications in natural language processing (NLP), including conversational agents, content creation tools, and sophisticated text analysis. 
We will see how such a model deals with analysing and predicting time series data.


\section{The Transformer}

The Transformer is a type of neural network architecture introduced in the seminal 2017 paper \textit{"Attention is All You Need"} by Vaswani et al. \cite{attention_is_all_you_need}. It has since become foundational for many natural language processing (NLP) models due to its efficiency and effectiveness in handling data sequences, such as text.

\subsection{Components}

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{"pictures/the-transformer-model-architecture.png"} % TODO: better diagram
	\caption{The Transformer model architecture} % TODO: better description - every part of diagram has to be explained
	\label{fig:transformer_architecture_fig}
\end{figure}

The Transformer (fig \ref{fig:transformer_architecture_fig}) consists of the following components:

\begin{enumerate}
	\item \textbf{Input Embedding}: Converts input tokens into vectors
	\item \textbf{Positional Encoding}: Adds positional information to the embeddings to retain the order of the sequence. This encoding is another high-dimensional vector.
	\item \textbf{Encoder} (on the left): % TODO: reference the picture
	      \begin{itemize}
		      \item Consists of N = 6 layers.
		      \item Each layer has two sub-layers:
		            \begin{itemize}
			            % TODO: multiple tokens - all? some? be precise
			            \item \textbf{Multi-Head Attention}: Applies attention mechanism over the input. Calculates the weight or importance of each token in its surrounding context (\textit{Attention}). Does so independently for multiple tokens at once (\textit{Multi-Head}). The results are concatenated and linearly transformed. % TODO: what does it mean linearly transformed? What for? To what?
			            \item \textbf{Feed Forward}: Applies a fully connected feed-forward neural network.
		            \end{itemize}
		      \item \textbf{Add \& Norm}: Residual connections followed by layer normalization after each sub-layer.
	      \end{itemize}
	\item \textbf{Decoder} (on the right): % TODO: reference the picture
	      \begin{itemize}
		      \item Also consists of N = 6 layers. % TODO: descibe that this is from article.
		      \item Each layer has three sub-layers:
		            \begin{itemize}
			            % TODO: what does it mean 'masked'
			            \item \textbf{Masked Multi-Head Attention}: As in Encoder, but masked - ensures that the predictions for position i can depend only on the known outputs at positions less than i.
			                  % TODO: what does it mean 'attends'
			            \item \textbf{Multi-Head Attention}: Attends to the encoder’s output.
			            \item \textbf{Feed Forward}: Applies a fully connected feed-forward neural network.
		            \end{itemize}
		      \item \textbf{Add \& Norm}: Residual connections followed by layer normalization after each sub-layer.
	      \end{itemize}
	\item \textbf{Output Embedding}: Converts decoder output tokens into semantic vector space.
	      % TODO: the following is contrary to intuition and experience of using e.g. ChatGPT
	\item \textbf{Linear \& Softmax Layer}: Maps the decoder’s output to the probability distribution over the target vocabulary. The most probable token is the output.
\end{enumerate}




\section{LLaMA model}

\subsection{Introduction}
\textbf{LLaMA} or \textit{Large Language Model Meta AI} \cite{llama} is a collection of large language models developed by Meta AI.

\subsection{Features}

\begin{itemize}
	\item \textbf{Model Variants:} LLaMA is available in various sizes, offering flexibility for deployment in different environments. These variants range from models of 7 billion parameters in size up to models of 65 billion parameters in size. % TODO: how is it for LLaMA-3?

	\item \textbf{Training Data:} The model has been trained on 1.4 trillion tokens of data from several sources, including CommonCrawl and Github, Wikipedia (Table 1. in \cite{llama}). It therefore has an enormous and domain diverse range of input data.

	      % \item \textbf{Application Scope:} Due to the possibility of using different sizes, LLaMA is suited for a variety of applications, such as conversational agents, content generation, summarization, and more advanced tasks like sentiment analysis and machine translation.

	\item \textbf{Accessibility:} The code that can be used to run the model has been publicly released under the open-source GPLv3 license \cite{llama_code}.
\end{itemize}

\subsection{LLaMA-2}
\textbf{LLaMA-2} \cite{llama2} is an improved version of LLaMA, with similar model sizes. It has the same architecture as LLaMA-1, but was trained on a much larger set of data (~2 trillion tokens). It also has doubled context length of 4096 tokens.

\section{Time-series Embedding}
We now present the technique we've used for using a vanilla (not fine-tuned) LLaMA-2 model to predict time series data.
% TODO: parantheses - what is the process? fine-tuning?
The main idea involves using a framework around a frozen LLM (i.e. one that is not changed in the process) that transforms input time series data into a text representation the LLM can then work on. Its output is then converted into a prediction. The idea is due to an article by Jin et al. (2024) \cite{reprogramming_llm}.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{"pictures/prompt-embedding.png"} % TODO: poor quality of picture - should be better explained
	\caption{The model framework of TIME-LLM. Given an input time series, we first tokenize and
		embed it via (1) patching along with a (2) customized embedding layer. (3) These patch embeddings
		are then reprogrammed with condensed text prototypes to align two modalities. To augment the
		LLM’s reasoning ability, (4) additional prompt prefixes are added to the input to direct the transformation
		of input patches. (5) The output patches from the LLM are projected to generate the forecasts.}
	\label{fig:prompt_embedding_fig}
\end{figure}



What follows is a more detailed description.
% TODO: This paragraph/problem may also be described earlier, e.g. in introduction.
We consider the following problem: given a sequence of historical observations \(X \in \R^{N\times T}\)
consisting of \(N\) different 1-dimensional variables across \(T\) time steps, we aim to reprogram a large
language model \(f(\cdot)\) to understand the input time series and accurately forecast the readings at \(H\) future time steps, denoted by \(\hat{Y} \in \R^{N\times H}\) , with the overall objective to minimize the mean square errors between the expected outputs \(Y\) and predictions, i.e., \(\frac1H \sum_{h=1}^H \| \hat{Y}_h - Y_h \|_F^2 \).

The method encompasses three main components: (1) input transformation, (2) a pre-trained and frozen LLM, and (3) output projection. Initially, a multifeature time series is partitioned into \(N\) unifeature time series, which are subsequently processed independently (Nie et al., 2023) \cite{nie_et_al}.
% TODO: normalization, patching, embedding - explain
The i-th series is denoted as \(X(i) \in \R^{1\times T}\) , which undergoes normalization, patching, and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities.
Then, we augment the LLM’s time series reasoning ability by prompting it together with the transformed series to generate output representations, which are projected to the final forecasts \(\hat{Y}^{(i)} \in \R^{1\times H}\) .

% TODO: what is cross-modality data? Explain, give examples.
We note that only the parameters of the lightweight input transformation and output projection are updated, while the backbone language model is frozen.
In contrast to vision-language and other multimodal language models, which usually fine-tune with paired cross-modality data, this use of model is directly optimized and becomes readily available with only a small set of time series and a few  training epochs, maintaining high efficiency and imposing fewer resource constraints compared to building large domain-specific models from scratch or fine-tuning them.

\section{Our methodology}
\section{Results}
