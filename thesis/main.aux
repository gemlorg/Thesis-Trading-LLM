\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{5}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Overview}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{5}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline}{5}{section.1.3}\protected@file@percent }
\citation{apple_source}
\citation{electricity_source}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminary definitions \& guidelines}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:preliminary-definitions}{{2}{7}{Preliminary definitions \& guidelines}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Macros}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Notation}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Datasets}{7}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Explanation of the datasets}{8}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Metrics}{8}{section.2.4}\protected@file@percent }
\citation{overfitting}
\citation{random_forest}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Other models}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:other-models}{{3}{9}{Other models}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Noisy (roughly linear) data is fitted to a linear function and a polynomial function. Although the polynomial function is a perfect fit, the linear function can be expected to generalize better: if the two functions were used to extrapolate beyond the fitted data, the linear function should make better predictions.}}{9}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overfitting}{{3.1}{9}{Noisy (roughly linear) data is fitted to a linear function and a polynomial function. Although the polynomial function is a perfect fit, the linear function can be expected to generalize better: if the two functions were used to extrapolate beyond the fitted data, the linear function should make better predictions}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Random forest}{9}{section.3.1}\protected@file@percent }
\citation{logistic_regression}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Logistic regression}{10}{section.3.2}\protected@file@percent }
\citation{support_vector_machine}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Support vector machine}{11}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Parameters}{11}{subsection.3.3.1}\protected@file@percent }
\citation{multilayer_perceptron}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Multilayer Perceptron}{12}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Structure of an MLP}{12}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Forward Propagation}{12}{subsection.3.4.2}\protected@file@percent }
\citation{convolutional_neural_network}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Backpropagation and Training}{13}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Convolutional neural network}{13}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Architecture of Convolutional Neural Networks}{13}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Convolutional Layer}{13}{section*.3}\protected@file@percent }
\citation{cnn_diagram_source}
\@writefile{toc}{\contentsline {subsubsection}{Activation Function}{14}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pooling Layer}{14}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fully Connected Layer}{14}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Output Layer}{14}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A simple diagram illustrating the different layers of the network working on an example input image.}}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:cnn-diagram}{{3.2}{14}{A simple diagram illustrating the different layers of the network working on an example input image}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Residual Neural Network}{14}{section.3.6}\protected@file@percent }
\citation{language_models}
\citation{token_embeddings}
\citation{llmintro}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Large Language Model}{15}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:large-language-model}{{4}{15}{Large Language Model}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Vocabulary}{15}{section.4.1}\protected@file@percent }
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Overview}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Transformer}{16}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Components}{16}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The Transformer model architecture (figure from \cite  {attention_is_all_you_need}). Both inputs and outputs are embedded and concatinated (\(\oplus \)) with their positional encodings - those are then fed into encoder and decoder respectivaly. The left part is one encoder layer. The whole encoder is made up of \(N\) of these layers stacked (output of one is input of the next). The output of the last encoder layer is used in all \(N\) decoder layers (on the right), which are likewise stacked. The output of the decoder (stack of decoder layers) is used for prediction.}}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:transformer_architecture_fig}{{4.1}{17}{The Transformer model architecture (figure from \cite {attention_is_all_you_need}). Both inputs and outputs are embedded and concatinated (\(\oplus \)) with their positional encodings - those are then fed into encoder and decoder respectivaly. The left part is one encoder layer. The whole encoder is made up of \(N\) of these layers stacked (output of one is input of the next). The output of the last encoder layer is used in all \(N\) decoder layers (on the right), which are likewise stacked. The output of the decoder (stack of decoder layers) is used for prediction}{figure.caption.9}{}}
\citation{llama}
\citation{llama}
\citation{llama_code}
\citation{llama2}
\citation{reprogramming_llm}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}LLaMA model}{18}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Introduction}{18}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Features}{18}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}LLaMA-2}{18}{subsection.4.4.3}\protected@file@percent }
\citation{nie_et_al}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Time-series Embedding}{19}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Our methodology}{19}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Data Preprocessing}{19}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Embedding}{20}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Patching}{20}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Body and Output Projection}{20}{subsection.4.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Model Parameters}{20}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Impact on Results}{20}{subsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Overfitting Concerns}{21}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Prompt Engineering}{21}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Indicators Used}{23}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Possible Improvements}{24}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Underlying LLM}{24}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.4}Larger Training Set}{24}{subsection.4.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Results}{24}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Main results}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results}{{5}{25}{Main results}{chapter.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{29}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{29}{Conclusion}{chapter.6}{}}
\bibcite{apple_source}{1}
\bibcite{electricity_source}{2}
\bibcite{overfitting}{3}
\bibcite{random_forest}{4}
\bibcite{logistic_regression}{5}
\bibcite{support_vector_machine}{6}
\bibcite{multilayer_perceptron}{7}
\bibcite{convolutional_neural_network}{8}
\bibcite{cnn_diagram-source}{9}
\bibcite{residual_neural_network}{10}
\bibcite{token_embeddings}{11}
\bibcite{language_models}{12}
\bibcite{llmintro}{13}
\bibcite{attention_is_all_you_need}{14}
\bibcite{llama}{15}
\bibcite{llama_code}{16}
\bibcite{llama2}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{31}{appendix*.10}\protected@file@percent }
\bibcite{reprogramming_llm}{18}
\bibcite{nie_et_al}{19}
\gdef \@abspage@last{34}
